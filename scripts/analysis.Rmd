---
title: "Weighted Analyses"
author: "steppe"
date: "2022-12-12"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r Load Libraries}
library(tidyverse)
library(spsurvey)
source('functions.R')
```


```{r Load and subset data to field office, warning = F}
pr <- '../data/raw'
f <- list.files(pr, recursive = T)

UFO_borders <- st_read(file.path(pr, f[grep('*admu.*shp$', f)]), quiet = T) %>% 
  filter(str_detect(ADMU_NAME, 'UNCOMPAHGRE')) %>% 
  st_transform(4269) %>% 
  select(geometry)

summaries <- read.csv(file.path(pr, f[grep('*Hub*', f)])) %>% 
  filter(State == 'CO') %>%  # reduce to your state of interest 
  select(-X, -Y, -State:-EcolSiteName, -DateVisited, -Purpose:-DBKey, -PrimaryKey, 
         -starts_with("RH_"), -starts_with('Sagebrush'), -starts_with('Spp_')) %>% 
  st_as_sf(coords = c(x = 'Longitude_NAD83', y = 'Latitude_NAD83'), crs = 4269, remove = F) %>%  # now make spatial
  st_intersection(UFO_borders, .) %>% 
  select_if(~sum(!is.na(.)) > 0)

rm(UFO_borders)
```


```{r Filter TerraDat to grab only base AIM plots and add post strat names}

baseplots <- st_read(file.path(pr, f[grep('*Design.*shp$', f)]), quiet = T) %>% 
  filter(!str_detect(PANEL, 'Over')) %>% 
  pull(PLOTID)

summaries <- summaries %>% 
  mutate(PlotID = str_replace(PlotID, 'Other', 'OT'),
         PlotID = str_replace(PlotID, 'Rip', 'RI')) %>% 
  filter(PlotID %in% baseplots)

restrat <- st_read(file.path(pr, f[grep('reclassified.*shp$', f)]), quiet = T) %>% 
  st_drop_geometry() %>% 
  select(-Plot)

summaries <- left_join(summaries, restrat, by = 'PlotKey')
  
rm(f, pr, baseplots, restrat)
```


```{r Filter dataset for only variables which have variation}

summary_cols_not_applicable <- summaries %>% # find columns, where fractional cover
  select(where(~ is.numeric(.x) && sum(.x, na.rm = T) == 0)) %>% # in all rows
  colnames() # is equal to 0, these are func. grps. not present in the area of analysis

NA_per_col <- summaries %>% # we can identify columns which have MANY NA values in them
  st_drop_geometry() %>% # perhaps if > 10% of your values per variables are NA
  summarise(across(.cols = everything(),  ~ (sum(is.na(.))))/nrow(.) * 100) %>% 
  select(where(~ .x >= 10)) # you want to take a look at them - Ours are not too bad.

zero_per_col <- summaries %>% # we can identify columns which have MANY 0 values
  st_drop_geometry() %>% # perhaps > 75 percent 0 is too many zereos!
  summarise(across(.cols = everything(), ~ sum(.x == 0))/nrow(.) * 100)  %>%
  mutate(across(.cols = everything(), ~ if_else(is.na(.x), 0, .x))) %>% 
  select(where(~ .x >= 75))

Variation <- summaries %>% # IDENTIFY COLUMNS with little to no variation in the 
  st_drop_geometry() %>% # measured variables, if we see var == 0, all measurements
  select(where(~ is.numeric(.x))) %>%  # were the same but not caught by 'summary cols not applicable'
  summarise(across(.cols = everything(), ~ # we do not have this problem which may
                     var(scales::rescale(., to = c(0,1), na.rm = T), na.rm = T))) %>% 
  t() %>% # arise if we always have the same response of an encoded categorical variable
  data.frame() %>% # we can still see which of the variables have littler variance
  rownames_to_column('Variable') %>% rename(Variance = '.') %>% # here
  filter(!Variable %in% c('OBJECTID', 'Latitude_NAD83', 'Longitude_NAD83', 'YearEstablished'))

hist(Variation$Variance) # the values far left are seldom recorded, e.g. we got rid of vagrant lichen years ago!

summaries1 <- summaries %>% 
  st_drop_geometry() %>% 
  filter(is.na(SoilStability_All)) %>% 
  mutate(across(starts_with('SoilStability'), ~ 999))
summaries <- summaries %>% 
  filter(!PlotID %in% summaries1$PlotID) %>% 
  bind_rows(., summaries1)

summaries <- summaries %>% 
  select(-any_of(summary_cols_not_applicable)) %>% 
  mutate(DateEstablished = as.Date(DateEstablished), # convert to date format
         YearEstablished = as.numeric(format(DateEstablished,'%Y'), .)
         )

rm(summary_cols_not_applicable, NA_per_col, Variation, summaries1, zero_per_col)
```


```{r Calculate Percent of each Stratum meeting standards}

sum1 <- summaries %>% 
  separate(PlotID, into = 'Stratum', sep = '-', remove = F, extra = 'drop') %>% 
  mutate(Stratum = factor(Stratum)) %>% 
  st_transform(26912) %>% 
  filter(PlotID != 'GR-021')

v <- data.frame(weight = wakefield::probs(length(unique(sum1$Stratum))),
           Stratum = unique(sum1$Stratum))
sum1 <- left_join(sum1, v, by = 'Stratum')

contVars2Calc <- sum1 %>% 
  select(BareSoilCover:NumSpp_PreferredForb ) %>% 
  st_drop_geometry() %>% 
  colnames(.)

contVarsInter <- vector(mode = 'list', length = length(contVars2Calc))
for (i in seq(contVars2Calc)){
  contVarsInter[[i]] <- prcnt_meeting(x = sum1, grp_var1 = 'Veg_type',  variables = contVars2Calc[i],  
                     pctval = c(70, 80), conf = 80 )
}
contVarsInter <- bind_rows(contVarsInter)


# recode the upper and lower end of confidence intervals for values which represent
# percentages, such as cover

cover <- contVarsInter %>% 
  filter(str_detect(Indicator, 'Cover')) %>% 
  mutate(across(starts_with('LCB'), ~  if_else(.x < 0, 0, .x))) %>% 
  mutate(across(starts_with('UCB'), ~  if_else(.x > 100, 100, .x)))
contVarsInter %>% 
  filter(!str_detect(Indicator, 'Cover')) %>% 
  bind_rows(cover)

rm(i, cover, prcnt_meeting)
```


```{r Calculate Mean Indicator Values}

contVarsMeans <- cont_analysis(dframe = sum1, subpops = 'Veg_type', vars = contVars2Calc, 
                               statistics = 'Mean', conf = 80)
contVarsMeans <- contVarsMeans['Mean'] %>% bind_rows()

cover <- contVarsMeans %>% 
  filter(str_detect(Indicator, 'Cover')) %>% 
  mutate(across(starts_with('LCB'), ~  if_else(.x < 0, 0, .x))) %>% 
  mutate(across(starts_with('UCB'), ~  if_else(.x > 100, 100, .x)))
contVarsMeans <- contVarsMeans %>% 
  filter(!str_detect(Indicator, 'Cover')) %>% 
  bind_rows(cover)

rm(cover, warn_df)
```



```{r}

```

# Reference
